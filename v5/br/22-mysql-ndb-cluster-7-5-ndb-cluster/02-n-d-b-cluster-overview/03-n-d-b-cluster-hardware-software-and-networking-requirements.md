### 21.2.3 Requisitos de hardware, software e redes do cluster NDB

Uma das principais vantagens do NDB Cluster é que ele pode ser executado em hardware comum e não tem requisitos incomuns nesse sentido, exceto para grandes quantidades de RAM, devido ao fato de que todo o armazenamento de dados em tempo real é feito na memória. (É possível reduzir essa exigência usando tabelas de dados de disco—consulte Seção 21.6.11, “Tabelas de Dados de Disco do NDB Cluster”, para obter mais informações sobre essas tabelas.) Naturalmente, CPUs múltiplas e mais rápidas podem melhorar o desempenho. Os requisitos de memória para outros processos do NDB Cluster são relativamente baixos.

Os requisitos de software para o NDB Cluster também são modestos. Os sistemas operacionais hospedeiros não exigem módulos, serviços, aplicativos ou configurações incomuns para suportar o NDB Cluster. Para os sistemas operacionais suportados, uma instalação padrão deve ser suficiente. Os requisitos de software do MySQL são simples: tudo o que é necessário é uma versão de produção do NDB Cluster. Não é estritamente necessário compilar o MySQL você mesmo apenas para poder usar o NDB Cluster. Assumemos que você está usando os binários apropriados para sua plataforma, disponíveis na página de downloads do software do NDB Cluster em <https://dev.mysql.com/downloads/cluster/>.

Para a comunicação entre os nós, o NDB Cluster suporta a rede TCP/IP em qualquer topologia padrão, e o mínimo esperado para cada host é um cartão Ethernet padrão de 100 Mbps, além de um switch, hub ou roteador para fornecer conectividade de rede para o conjunto como um todo. Recomendamos fortemente que um NDB Cluster seja executado em sua própria sub-rede que não seja compartilhada com máquinas que não fazem parte do conjunto, pelas seguintes razões:

- **Segurança.** As comunicações entre os nós do NDB Cluster não são criptografadas ou protegidas de nenhuma forma. O único meio de proteger as transmissões dentro de um NDB Cluster é executar o NDB Cluster em uma rede protegida. Se você pretende usar o NDB Cluster para aplicações web, o cluster deve residir definitivamente atrás do seu firewall e não na Zona Desmilitarizada (DMZ) ou em outro lugar.

  Para obter mais informações, consulte Seção 21.6.18.1, “Problemas de segurança e de rede do cluster NDB”.

- **Eficiência.** Configurar um NDB Cluster em uma rede privada ou protegida permite que o cluster faça uso exclusivo da largura de banda entre os hosts do cluster. Usar um switch separado para o seu NDB Cluster não só ajuda a proteger contra o acesso não autorizado aos dados do NDB Cluster, como também garante que os nós do NDB Cluster estejam protegidos contra interferências causadas por transmissões entre outros computadores na rede. Para maior confiabilidade, você pode usar switches e placas duplas para remover a rede como um único ponto de falha; muitos drivers de dispositivos suportam o failover para tais links de comunicação.

**Comunicação em rede e latência.** O NDB Cluster requer comunicação entre os nós de dados e os nós da API (incluindo os nós SQL), bem como entre os nós de dados e outros nós de dados, para executar consultas e atualizações. A latência da comunicação entre esses processos pode afetar diretamente o desempenho e a latência observados das consultas dos usuários. Além disso, para manter a consistência e o serviço mesmo diante da falha silenciosa de nós, o NDB Cluster utiliza mecanismos de batida de coração e tempo de espera que tratam uma perda prolongada de comunicação de um nó como falha do nó. Isso pode levar à redução da redundância. Lembre-se de que, para manter a consistência dos dados, um NDB Cluster é desligado quando o último nó de um grupo de nós falha. Assim, para evitar aumentar o risco de um desligamento forçado, as interrupções na comunicação entre os nós devem ser evitadas sempre que possível.

O falecimento de um nó de dados ou de uma API resulta no cancelamento de todas as transações não confirmadas que envolvem o nó falhado. A recuperação do nó de dados requer a sincronização dos dados do nó falhado a partir de um nó de dados sobrevivente e o restabelecimento dos registros de redo e de ponto de verificação baseados em disco, antes que o nó de dados volte a funcionar. Essa recuperação pode levar algum tempo, durante o qual o clúster opera com redundância reduzida.

O Heartbeat depende da geração oportuna de sinais de batimento cardíaco por todos os nós. Isso pode não ser possível se o nó estiver sobrecarregado, tiver CPU insuficiente devido à compartilhamento com outros programas ou estiver sofrendo atrasos devido ao swapping. Se a geração do Heartbeat for suficientemente atrasada, outros nós tratam o nó que demora a responder como falhado.

Esse tratamento de um nó lento como falho pode ou não ser desejável em algumas circunstâncias, dependendo do impacto da operação lenta do nó no resto do clúster. Ao definir valores de tempo de espera, como `HeartbeatIntervalDbDb` e `HeartbeatIntervalDbApi` para o NDB Cluster, é necessário ter cuidado para alcançar a detecção rápida, a falha e o retorno ao serviço, evitando falsos positivos potencialmente caros.

Quando se espera que as latências de comunicação entre os nós de dados sejam maiores do que o esperado em um ambiente de LAN (da ordem de 100 µs), os parâmetros de tempo de espera devem ser aumentados para garantir que quaisquer períodos permitidos de latência estejam dentro dos tempos de espera configurados. Aumentar os tempos de espera dessa maneira tem um efeito correspondente no tempo máximo para detectar a falha e, portanto, no tempo para a recuperação do serviço.

Os ambientes LAN geralmente podem ser configurados com baixa latência estável e de forma que possam oferecer redundância com failover rápido. Falhas individuais de link podem ser recuperadas com latência mínima e controlada visível no nível TCP (onde o NDB Cluster normalmente opera). Ambientes WAN podem oferecer uma gama de latências, bem como redundância com tempos de failover mais lentos. Falhas individuais de link podem exigir alterações de rota para se propagar antes que a conectividade de ponta a ponta seja restaurada. No nível TCP, isso pode aparecer como grandes latências em canais individuais. A latência TCP observada no pior dos cenários nesses casos está relacionada ao tempo máximo para a camada IP redirecionar ao redor das falhas.
