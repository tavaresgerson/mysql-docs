### 21.2.3 Requisitos de Hardware, Software e Rede do NDB Cluster

Uma das forças do NDB Cluster é que ele pode ser executado em hardware comercial e não tem requisitos incomuns a esse respeito, exceto por grandes quantidades de RAM, devido ao fato de que todo o armazenamento de dados ativos é feito na memória. (É possível reduzir esse requisito usando tabelas Disk Data — consulte [Section 21.6.11, “NDB Cluster Disk Data Tables”](mysql-cluster-disk-data.html "21.6.11 NDB Cluster Disk Data Tables"), para obter mais informações sobre elas.) Naturalmente, múltiplos CPUs mais rápidos podem aumentar a performance. Os requisitos de memória para outros processos do NDB Cluster são relativamente pequenos.

Os requisitos de software para o NDB Cluster também são modestos. Os sistemas operacionais host não requerem módulos, serviços, aplicativos ou configurações incomuns para dar suporte ao NDB Cluster. Para os sistemas operacionais suportados, uma instalação padrão deve ser suficiente. Os requisitos de software do MySQL são simples: tudo o que é necessário é uma versão de produção do NDB Cluster. Não é estritamente necessário compilar o MySQL você mesmo apenas para poder usar o NDB Cluster. Presumimos que você esteja usando os binários apropriados para sua plataforma, disponíveis na página de downloads de software do NDB Cluster em <https://dev.mysql.com/downloads/cluster/>.

Para a comunicação entre nodes, o NDB Cluster suporta redes TCP/IP em qualquer topologia padrão, e o mínimo esperado para cada host é uma placa Ethernet padrão de 100 Mbps, além de um switch, hub ou router para fornecer conectividade de rede para o Cluster como um todo. Recomendamos enfaticamente que um NDB Cluster seja executado em sua própria sub-rede, que não seja compartilhada com máquinas que não façam parte do Cluster, pelos seguintes motivos:

* **Segurança.** As comunicações entre os nodes do NDB Cluster não são criptografadas ou blindadas de forma alguma. O único meio de proteger as transmissões dentro de um NDB Cluster é executá-lo em uma rede protegida. Se você pretende usar o NDB Cluster para aplicações Web, o Cluster deve definitivamente residir atrás do seu firewall e não na Zona Desmilitarizada da sua rede ([DMZ](http://compnetworking.about.com/cs/networksecurity/g/bldef_dmz.htm)) ou em outro lugar.

  Consulte [Section 21.6.18.1, “NDB Cluster Security and Networking Issues”](mysql-cluster-security-networking-issues.html "21.6.18.1 NDB Cluster Security and Networking Issues"), para obter mais informações.

* **Eficiência.** Configurar um NDB Cluster em uma rede privada ou protegida permite que o Cluster faça uso exclusivo de largura de banda entre os hosts do Cluster. Usar um switch separado para o seu NDB Cluster não apenas ajuda a proteger contra acesso não autorizado aos dados do NDB Cluster, mas também garante que os nodes do NDB Cluster estejam protegidos contra interferências causadas por transmissões entre outros computadores na rede. Para confiabilidade aprimorada, você pode usar switches duplos e placas duplas para remover a rede como um ponto único de falha; muitos drivers de dispositivo suportam failover para esses links de comunicação.

**Comunicação de rede e Latency.** O NDB Cluster requer comunicação entre data nodes e API nodes (incluindo SQL nodes), bem como entre data nodes e outros data nodes, para executar Queries e updates. A latency de comunicação entre esses processos pode afetar diretamente a performance observada e a latency das Queries do usuário. Além disso, para manter a consistência e o serviço apesar da falha silenciosa de nodes, o NDB Cluster usa mecanismos de heartbeating e timeout que tratam uma perda prolongada de comunicação de um node como falha do node. Isso pode levar à redução da redundância. Lembre-se de que, para manter a consistência dos dados, um NDB Cluster é desligado quando o último node em um grupo de nodes falha. Assim, para evitar aumentar o risco de um desligamento forçado, as interrupções na comunicação entre nodes devem ser evitadas sempre que possível.

A falha de um data ou API node resulta no abort de todas as transações não comitadas envolvendo o node com falha. A recuperação do data node requer a sincronização dos dados do node com falha a partir de um data node sobrevivente e o restabelecimento de logs de redo e checkpoint baseados em disco, antes que o data node retorne ao serviço. Essa recuperação pode levar algum tempo, durante o qual o Cluster opera com redundância reduzida.

O Heartbeating depende da geração oportuna de sinais de heartbeat por todos os nodes. Isso pode não ser possível se o node estiver sobrecarregado, tiver CPU da máquina insuficiente devido ao compartilhamento com outros programas, ou estiver enfrentando atrasos devido a swapping. Se a geração de heartbeat for suficientemente atrasada, outros nodes tratarão o node que está lento para responder como falho.

Esse tratamento de um node lento como falho pode ou não ser desejável em algumas circunstâncias, dependendo do impacto da operação lenta do node no restante do Cluster. Ao definir valores de timeout como [`HeartbeatIntervalDbDb`](mysql-cluster-ndbd-definition.html#ndbparam-ndbd-heartbeatintervaldbdb) e [`HeartbeatIntervalDbApi`](mysql-cluster-ndbd-definition.html#ndbparam-ndbd-heartbeatintervaldbapi) para o NDB Cluster, deve-se tomar cuidado para obter detecção rápida, failover e retorno ao serviço, evitando false positives potencialmente caros.

Onde as latencies de comunicação entre data nodes são esperadas como sendo mais altas do que o esperado em um ambiente LAN (na ordem de 100 µs), os parâmetros de timeout devem ser aumentados para garantir que quaisquer períodos permitidos de latency estejam bem dentro dos timeouts configurados. Aumentar os timeouts dessa forma tem um efeito correspondente no tempo de pior caso para detectar falhas e, portanto, no tempo de recuperação do serviço.

Ambientes LAN podem ser tipicamente configurados com baixa latency estável, e de forma que possam fornecer redundância com failover rápido. Falhas de link individuais podem ser recuperadas com latency mínima e controlada visível no nível TCP (onde o NDB Cluster normalmente opera). Ambientes WAN podem oferecer uma variedade de latencies, bem como redundância com tempos de failover mais lentos. Falhas de link individuais podem exigir que as alterações de rota se propaguem antes que a conectividade ponta a ponta seja restaurada. No nível TCP, isso pode parecer como grandes latencies em canais individuais. A latency TCP de pior caso observada nesses cenários está relacionada ao tempo de pior caso para que a camada IP redirecione (reroute) o tráfego em torno das falhas.