### 25.2.3 Requisitos de Hardware, Software e Networking do NDB Cluster

Uma das principais vantagens do NDB Cluster é que ele pode ser executado em hardware comum e não possui requisitos incomuns nesse sentido, exceto para grandes quantidades de RAM, devido ao fato de que todo o armazenamento de dados em tempo real é feito na memória. (É possível reduzir esse requisito usando tabelas de Dados de Disco—consulte a Seção 25.6.11, “Tabelas de Dados de Disco do NDB Cluster”, para obter mais informações sobre isso.) Você pode obter informações sobre o uso de memória pelos nós de dados visualizando a tabela `ndbinfo.memoryusage`, ou a saída do comando `REPORT MemoryUsage` no cliente **ndb\_mgm**. Para informações sobre a memória usada pelas tabelas `NDB`, você pode consultar a tabela `ndbinfo.memory_per_fragment`.

Aumentar o número de CPUs, usar CPUs mais rápidas ou ambos, nos computadores que hospedam os nós de dados geralmente pode melhorar o desempenho do NDB Cluster. Os requisitos de memória para os processos do cluster, exceto os nós de dados, são relativamente pequenos.

Os requisitos de software para o NDB Cluster também são modestos. Os sistemas operacionais hospedeiros não requerem módulos, serviços, aplicações ou configurações incomuns para suportar o NDB Cluster. Para os sistemas operacionais suportados, uma instalação padrão deve ser suficiente. Os requisitos de software do MySQL são simples: tudo o que é necessário é uma versão de produção do NDB Cluster. Não é estritamente necessário compilar o MySQL você mesmo apenas para poder usar o NDB Cluster. Assumemos que você está usando os binários apropriados para sua plataforma, disponíveis na página de downloads do software NDB Cluster em <https://dev.mysql.com/downloads/cluster/>.

Para a comunicação entre nós, o NDB Cluster suporta a rede TCP/IP em qualquer topologia padrão, e o mínimo esperado para cada host é um cartão Ethernet padrão de 100 Mbps, além de um switch, hub ou roteador para fornecer conectividade de rede para o clúster como um todo.

Recomendamos fortemente que um NDB Cluster seja executado em sua própria sub-rede que não seja compartilhada com máquinas que não fazem parte do clúster; usar uma rede privada ou protegida permite que o clúster faça uso exclusivo da largura de banda entre os hosts do clúster. Usar um switch separado para a instalação do seu NDB Cluster não só ajuda a proteger contra o acesso não autorizado aos dados armazenados no clúster, mas também garante que os nós do clúster estejam protegidos contra interferências causadas por transmissões entre outros computadores na rede. Para maior confiabilidade, você pode usar switches e cartões duplos para remover a rede como um único ponto de falha; muitos drivers de dispositivos suportam o failover para tais links de comunicação.

O `NDB` suporta arquivos e sistemas de arquivos criptografados ao vivo e de backup, conforme discutido na Seção 25.6.19.4, “Criptografia de Sistema de Arquivos para NDB Cluster”. A Seção 25.6.19.5, “Criptografia de Link TLS para NDB Cluster”, fornece informações sobre a habilitação do suporte para conexões criptografadas entre nós. Os backups criptografados podem ser lidos por muitos programas de linha de comando do `NDB`, incluindo **ndb\_restore**, **ndbxfrm**, **ndb\_print\_backup\_file** e **ndb\_mgm**. Veja a Seção 25.6.8.2, “Usando o Cliente de Gerenciamento do NDB Cluster para Criar um Backup”, para mais informações sobre a criação de backups criptografados.

O NDB Cluster também suporta conexões de rede criptografadas entre nós; veja a Seção 25.6.19.4, “Criptografia de Sistema de Arquivos para NDB Cluster”, para detalhes.

**Comunicação em rede e latência.** O NDB Cluster requer comunicação entre os nós de dados e os nós da API (incluindo os nós SQL), bem como entre os nós de dados e outros nós de dados, para executar consultas e atualizações. A latência da comunicação entre esses processos pode afetar diretamente o desempenho e a latência observados das consultas dos usuários. Além disso, para manter a consistência e o serviço mesmo com a falha silenciosa de nós, o NDB Cluster utiliza mecanismos de batida de coração e tempo de espera que tratam uma perda prolongada de comunicação de um nó como falha do nó. Isso pode levar à redução da redundância. Lembre-se de que, para manter a consistência dos dados, um NDB Cluster é desligado quando o último nó de um grupo de nós falha. Assim, para evitar aumentar o risco de um desligamento forçado, as interrupções na comunicação entre os nós devem ser evitadas sempre que possível.

A falha de um nó de dados ou de API resulta no cancelamento de todas as transações não confirmadas que envolvem o nó falhado. A recuperação do nó de dados requer a sincronização dos dados do nó falhado de um nó de dados sobrevivente e o restabelecimento dos logs de redo e ponto de verificação baseados em disco, antes que o nó de dados volte a funcionar. Essa recuperação pode levar algum tempo, durante o qual o Cluster opera com redundância reduzida.

A batida de coração depende da geração oportuna de sinais de batida de coração por todos os nós. Isso pode não ser possível se o nó estiver sobrecarregado, tiver CPU insuficiente devido à compartilhamento com outros programas ou estiver enfrentando atrasos devido ao swapping. Se a geração de batida de coração for suficientemente atrasada, outros nós tratam o nó que é lento para responder como falhado.

Esse tratamento de um nó lento como falho pode ser ou não desejável em algumas circunstâncias, dependendo do impacto da operação lenta do nó no resto do clúster. Ao definir valores de tempo de espera, como `HeartbeatIntervalDbDb` e `HeartbeatIntervalDbApi` para o NDB Cluster, é necessário ter cuidado para garantir a detecção rápida, o failover e o retorno ao serviço, evitando possíveis falsos positivos caros.

Quando se espera que as latências de comunicação entre os nós de dados sejam maiores do que o esperado em um ambiente LAN (da ordem de 100 µs), os parâmetros de tempo de espera devem ser aumentados para garantir que quaisquer períodos permitidos de latência estejam dentro dos tempos de espera configurados. Aumentar os tempos de espera dessa maneira tem um efeito correspondente no tempo máximo para detectar a falha e, portanto, no tempo para a recuperação do serviço.

Ambientes LAN podem ser configurados com latências estáveis e baixas, e de forma que possam fornecer redundância com failover rápido. Falhas individuais de link podem ser recuperadas com latência mínima e controlada visível no nível TCP (onde o NDB Cluster normalmente opera). Ambientes WAN podem oferecer uma gama de latências, bem como redundância com tempos de failover mais lentos. Falhas individuais de link podem exigir alterações de rota para se propagar antes que a conectividade ponto a ponto seja restaurada. No nível TCP, isso pode aparecer como grandes latências em canais individuais. A latência TCP observada no pior dos casos nesses cenários está relacionada ao tempo máximo para a camada IP redirecionar ao redor das falhas.